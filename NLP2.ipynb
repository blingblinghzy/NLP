{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **November 25, 2022**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $50$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  5 |              5 |          10 |\n",
    "| 2    |                  - |             10 |          10 |\n",
    "| 3    |                  5 |              5 |          10 |\n",
    "| 4    |                  5 |              5 |          10 |\n",
    "| 5    |                  5 |              5 |          10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  670k  100  670k    0     0   117k      0  0:00:05  0:00:05 --:--:--  101k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  168k  100  168k    0     0   103k      0  0:00:01  0:00:01 --:--:--  103k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  208k  100  208k    0     0  18057      0  0:00:11  0:00:11 --:--:-- 22172\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing (10 Marks)\n",
    "\n",
    "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "\n",
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "Data preprocessing mainly includes the following steps, in which all kinds of dirty data are processed:\n",
    "1. Data quality assessment.\n",
    "Take a good look at your data and get an idea of its overall quality, relevance to your project, and consistency. There are a number of data anomalies and inherent problems to look out for in almost any data set, for example: Mismatched data types,Mixed data values,Data outliers,Missing data.\n",
    "2. Data Cleansing.\n",
    "In the process of data cleaning, missing values, outliers and duplicate values are mainly dealt with. The so-called cleaning refers to discarding, filling, replacing and reweighting data sets. To remove anomalies, correct errors and make up for the missing purpose.\n",
    "3. Data Transformation.\n",
    "Data standardization is a commonly used data preprocessing operation, which aims to convert data of different specifications to unified specifications or data of different distributions to a specific range, so as to reduce the impact of scale, features and distribution differences on the model. In addition to model calculation, standardized data also has the significance of directly calculating and generating composite indicators, which is a necessary step for weighted indicators.\n",
    "4. Feature Selection or Feature Combination.\n",
    "Feature selection is to select meaningful and helpful features from all features, so as to avoid the situation that all features must be imported into the model for training.\n",
    "Feature selection is completely independent of any machine learning algorithm. He chose features based on scores from various statistical tests and indicators of correlation.\n",
    "\n",
    "Data Preparation: from text to sentence\n",
    "Non-alphanumeric data removing: number, symbol, emoji, HTML tag…\n",
    "Lowercase and Miss-spelling normalization\n",
    "Tokenization: from sentence to word\n",
    "Stop Words removing\n",
    "Stemming and Lemmatization\n",
    "Bag of words: Tf-Idf\n",
    "Word Embedding: Word2Vec\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>\"One would hope if I search for a word in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>one would hope search word title app would sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>\"I would be beyond excited to get a response.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>would beyond excited get respons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>\"Just like the user can select apps that are a...</td>\n",
       "      <td>1</td>\n",
       "      <td>like user select apps allowed run background w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>\"Once you create a CoreIndependentInputSource ...</td>\n",
       "      <td>0</td>\n",
       "      <td>create coreindependentinputsource touch visual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>\"I Have problems with Contact class on Windows...</td>\n",
       "      <td>0</td>\n",
       "      <td>problems contact class windows 81 windowsphone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  label  \\\n",
       "0  train_0  \"One would hope if I search for a word in the ...      0   \n",
       "1  train_1     \"I would be beyond excited to get a response.\"      0   \n",
       "2  train_2  \"Just like the user can select apps that are a...      1   \n",
       "3  train_3  \"Once you create a CoreIndependentInputSource ...      0   \n",
       "4  train_4  \"I Have problems with Contact class on Windows...      0   \n",
       "\n",
       "                                               texts  \n",
       "0  one would hope search word title app would sho...  \n",
       "1                   would beyond excited get respons  \n",
       "2  like user select apps allowed run background w...  \n",
       "3  create coreindependentinputsource touch visual...  \n",
       "4  problems contact class windows 81 windowsphone...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk #Importing the Natural Language Toolkit\n",
    "import re #Importing the Regular Expressions\n",
    "from nltk.stem import WordNetLemmatizer #Importing the WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer #Importing the PorterStemmer\n",
    "from nltk.corpus import stopwords #Importing the stopwords\n",
    "from nltk.tokenize import word_tokenize #Importing the word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "def dataclean (words):\n",
    "\n",
    "    #Lowercasing\n",
    "    words=[word.lower() for word in words]\n",
    "  \n",
    "\n",
    "    #Stop Word Removal\n",
    "    stops = set(stopwords.words('english'))\n",
    "    nostop_words=[]\n",
    "\n",
    "    #remove stop words\n",
    "    for word in words:\n",
    "        sentence = [word for word in word.split() if not word in stops]\n",
    "        nostop_words.append(' '.join(sentence))\n",
    "    words = nostop_words\n",
    "\n",
    "    #Stemming\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    words=[porter_stemmer.stem(word=word) for word in words]\n",
    "   \n",
    "\n",
    "    #remove all punctuations\n",
    "    sentenceClean = []\n",
    "    for word in words:\n",
    "        word = re.sub(\"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\", \"\", word)\n",
    "        sentenceClean.append(word)\n",
    "    words = sentenceClean\n",
    "    \n",
    "    #Lemmatization\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #words=[lemmatizer.lemmatize(word=word,pos='v') for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "train_texts = dataclean(train_texts)\n",
    "test_texts = dataclean(test_texts)\n",
    "\n",
    "train_df['texts'] = train_texts\n",
    "test_df['texts'] = test_texts\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.58      0.67      1027\n",
      "           1       0.30      0.56      0.39       333\n",
      "\n",
      "    accuracy                           0.58      1360\n",
      "   macro avg       0.55      0.57      0.53      1360\n",
      "weighted avg       0.68      0.58      0.61      1360\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5764705882352941"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "train_count = vectorizer.fit_transform(train_texts).toarray()\n",
    "train_itIdf = transformer.fit_transform(train_count).toarray()\n",
    "\n",
    "test_count = vectorizer.transform(test_texts).toarray()\n",
    "test_itIdf = transformer.transform(test_count).toarray()\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "GNB = GaussianNB()\n",
    "# fit the model with data\n",
    "GNB.fit(train_itIdf, train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = GNB.predict(test_itIdf)  \n",
    "print(classification_report(test_labels, predictions))\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "   if label == prediction:\n",
    "        correct += 1 \n",
    "  \n",
    "        score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics (10 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "The most natural of the classification indicators is accuracy, which is the percentage of the total sample that predicts the correct results. However, When the data is balanced, accuracy is a good measure of evaluating our model. In other hand if data is imbalanced then accuracy is not a correct measure of evaluation.In business scenarios, most data won’t be balanced and so accuracy becomes poor measure of evaluation for our classification model.Different evaluation methods are used in different scenarios, and we can even use a combination of multiple dimensions to evaluate the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3924050632911392"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "\n",
    "\n",
    "  # check that labels and predictions are of same length\n",
    "    \n",
    "    assert len(labels) == len(predictions)\n",
    "    score = 0.0\n",
    "  \n",
    "  #################### EDIT BELOW THIS LINE #########################\n",
    "    \n",
    "    tp = 0\n",
    "    for gt, pred in zip(labels, predictions):\n",
    "        if gt == 1 and pred == 1:\n",
    "            tp +=1\n",
    "   \n",
    "    tn = 0\n",
    "    for gt, pred in zip(labels, predictions):\n",
    "        if gt == 0 and pred == 0:\n",
    "            tn +=1\n",
    "            \n",
    "    fp = 0\n",
    "    for gt, pred in zip(labels, predictions):\n",
    "        if gt == 0 and pred == 1:\n",
    "            fp +=1\n",
    "    \n",
    "    fn = 0\n",
    "    for gt, pred in zip(labels, predictions):\n",
    "        if gt == 1 and pred == 0:\n",
    "            fn +=1\n",
    "            \n",
    "    prec = tp/ (tp + fp)  \n",
    "    recall = tp/ (tp + fn)  \n",
    "    score = 2 * prec * recall/ (prec + recall)\n",
    "        \n",
    "  #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "    return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features (10 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 2 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "There are some preprocessing steps that can be executed before doing feature engineering.\n",
    "1 Dimensionless\n",
    "2 Standardization\n",
    "3 Interval scaling method\n",
    "4 Differences between standardization and normalization\n",
    "5 Binarization of quantitative features\n",
    "6 Dummy coding for qualitative features\n",
    "7 Calculation of missing values\n",
    "8 Data Transformation\n",
    "\n",
    "There are some feature options that we can choose.\n",
    "1 the Filter\n",
    "1.1 Variance selection method\n",
    "1.2 Correlation coefficient method\n",
    "1.3 Chi-square test\n",
    "1.4 Mutual Information method\n",
    "2 Wrapper\n",
    "2.1 Recursive feature elimination\n",
    "3 Embedded\n",
    "3.1 Feature selection method based on penalty term\n",
    "3.2 Feature selection method 4 Dimension reduction based on tree model\n",
    "4 Principal Component Analysis (PCA)\n",
    "5 Linear Discriminant Analysis (LDA)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfkzM3DRce14"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "# count number of characters \n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove = api.load('glove-wiki-gigaword-300')\n",
    "def textvectorize(dataframe):\n",
    "    \n",
    "    text_toks = []\n",
    "    for sent in dataframe[\"texts\"]:\n",
    "        # print(sent)\n",
    "        text_toks.append(word_tokenize(sent))\n",
    "        \n",
    "    all_text_vecs = []\n",
    "    oov = np.random.rand(1,300)    \n",
    "    \n",
    "    for toks in text_toks:\n",
    "        \n",
    "        text_vecs = []\n",
    "        \n",
    "        for tok in toks:\n",
    "            if tok in glove:\n",
    "                text_vecs.append(glove[tok])\n",
    "        # Special case where no word embedding is found\n",
    "            else:\n",
    "                text_vecs.append(oov)\n",
    "        if len(text_vecs) == 0:text_vecs.append(oov)       \n",
    "        all_text_vecs.append(text_vecs)    \n",
    "        \n",
    "\n",
    "# 1. Loop over each list of word embeddings per input\n",
    "    for text_vecs in all_text_vecs:\n",
    "       # 2. Vstack and take the mean of the tex_vecs\n",
    "        mean_pool = np.mean(np.vstack(text_vecs), axis=0)\n",
    "        # 3. Append the mean pooled vector to all_pooled_vecs\n",
    "        all_text_vecs.append(mean_pool)\n",
    "\n",
    "    dataframe[\"texts\"] = all_text_vecs  \n",
    "\n",
    "# 4. Update dataset with these pooled vectors\n",
    "textvectorize(train_df)\n",
    "textvectorize(test_df)\n",
    "\n",
    "# break out the encoded labels by train / test split\n",
    "train_Y = train_df[\"label\"]  \n",
    "test_Y  = test_df[\"label\"]\n",
    "\n",
    "X_train = np.vstack(train_df['texts'])\n",
    "X_test = np.vstack(test_df['texts'])\n",
    "\n",
    "print(f\"Input shapes X_train: {X_train.shape}, X_text: {X_test.shape}\")\n",
    "print(f\"Output shapes y_train: {y_train.shape}, y_text: {y_test.shape}\")\n",
    "\n",
    "# ... your code goes here\n",
    "GNB = GaussianNB()\n",
    "# fit the model with data\n",
    "GNB.fit(X_train, train_Y)\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = GNB.predict(test_Y)  \n",
    "# Evaluate on the test set.\n",
    "evaluate(test_Y,predictions)\n",
    "print(classification_report(test_Y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Kaggle Competition (10 marks)\n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
    "\n",
    "Read competition page for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9NZrBayoN4A",
    "outputId": "d2c338a4-f20f-429e-9c69-a4a7850de428"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]....*.*\n",
      "optimization finished, #iter = 5961\n",
      "obj = -1615.111333, rho = -1.129234\n",
      "nSV = 2986, nBSV = 1527\n",
      "Total nSV = 2986\n",
      "0.825\n"
     ]
    }
   ],
   "source": [
    "# Preparing submission for Kaggle\n",
    "#using svm methoed\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "StudentID = \"22210220_Hu\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "test_unseen['text'] = dataclean(test_unseen['text'])\n",
    "\n",
    "unseen_count = vectorizer.transform(test_unseen['text']).toarray()\n",
    "unseen_itIdf = transformer.transform(unseen_count).toarray()\n",
    "#predictions = GNB.predict(test_itIdf)  \n",
    "\n",
    "\n",
    "svc = SVC(kernel=\"linear\",verbose=True)\n",
    "# fit the model with data\n",
    "svc.fit(train_itIdf, train_labels)\n",
    "#predict on the test_unseen\n",
    "pre = svc.predict(test_itIdf)\n",
    "acc = accuracy(test_labels, pre)\n",
    "print(acc)\n",
    "predictions = svc.predict(unseen_itIdf)\n",
    "\n",
    "#calculation the accuracy and fi_score\n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
    "       \"Expected\": predictions}\n",
    "\n",
    "sub_df = pd.DataFrame(sub)\n",
    "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
    "# Please upload the file as a submission on the competition page\n",
    "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
    "sub_df.to_csv(f\"{22210220}.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "I chosed TF_IDF as the feature, and use SVM model to predict the unseen dataset. It gets the f1 score 0.825, which has been improved by using feature engineering and changing another model. Feature engineering is the process that takes raw data and transforms it into features that can be used to create a predictive model. The aim of feature engineering is to prepare an input data set that best fits algorithm as well as to enhance the performance of models.\n",
    "\n",
    "Framing The Problem Correctly: Using the right objective measures to estimate the accuracy of the output \n",
    "\n",
    "Inter-Dependencies Within The Model: The inherent, underlying structures in the organization’s data. Good structure always provides far better results. \n",
    "\n",
    "Once these things are considered when selecting or designing features, the advantages of feature engineering include:\n",
    "\n",
    "More flexibility and less complexity in models\n",
    "Faster processing\n",
    "Clear, easy-to-understand models\n",
    "Simpler models that are easier to maintain\n",
    "A better understanding of the underlying problem\n",
    "Better representation of all the available data that is helpful in characterizing the underlying problem\n",
    "\n",
    "Overall, the imput data is optimized.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
